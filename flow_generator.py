from nfstream import NFStreamer
import numpy as np
from nfstream.flow import NFlow
import pickle
import csv
import argparse

# LABELS = ["Benign", "DoS GoldenEye", "DoS Hulk", "DoS Slowhttptest", "DoS Slowloris"]
LABELS = ["Benign", "DoS GoldenEye"]


def run(network_interface_name):
    root_path = "/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/saved/"
    name = "dos_goldeneye_1800"

    # Load the trained classifier.
    clf_filepath = (
        root_path + f"/{name}_model.pkl"
    )  # Enter filepath to the trained classifier.
    clf = pickle.load(open(clf_filepath, "rb"))

    # Load the normalization scaler.
    scaler_filepath = (
        root_path + f"/{name}_scaler.pkl"
    )  # Enter filepath to the scaler used during the training of the classifier.
    scaler = pickle.load(open(scaler_filepath, "rb"))

    # Load the features that were used for training.
    features_filepath = (
        root_path + "/features.txt"
    )  # Enter filepath to a list with the name of the features used by the classifier.
    with open(features_filepath, "r") as file:
        features = [line.strip() for line in file]

    my_streamer = NFStreamer(
        source=network_interface_name,
        decode_tunnels=False,  # Disable GTP/CAPWAP/TZSP tunnels decoding.
        bpf_filter=None,
        promiscuous_mode=True,  # Enable promiscuous capture mode.
        snapshot_length=1536,  # Control packet slicing size (truncation) in bytes.
        idle_timeout=120,  # Flows that are idle (no packets received) for more than 120 seconds are expired.
        active_timeout=1800,  # Flows that are active for more than <active_timeout> seconds are expired.
        accounting_mode=3,  # Accounting mode that will be used to report bytes related features, 3: payload.
        udps=None,
        n_dissections=0,  # Disable L7 visibility feature.
        statistical_analysis=True,  # Enable post-mortem flow statistical analysis.
        splt_analysis=0,
        n_meters=0,
        max_nflows=0,
        performance_report=0,
        system_visibility_mode=0,
        system_visibility_poll_ms=100,
    )

    analyzed_flows = {
        "Benign": 0,
        "DoS GoldenEye": 0,
    }

    print(f"ML classification model: {name}")
    print(f"Flow features being used for ML classification: {features}")
    print(f"Analyzing flows on network interface {network_interface_name} ...")
    print()

    with open(f"./live_{name}.csv", "w", newline="") as csv_file:
        writer = csv.writer(csv_file)
        try:
            for flow in my_streamer:
                # Extract only the required features.
                flow_features = np.array(
                    [
                        flow.bidirectional_duration_ms,
                        flow.bidirectional_packets,
                        flow.bidirectional_bytes,
                        flow.src2dst_duration_ms,
                        flow.src2dst_packets,
                        flow.src2dst_bytes,
                        flow.dst2src_duration_ms,
                        flow.dst2src_packets,
                        flow.dst2src_bytes,
                        flow.bidirectional_min_ps,
                        flow.bidirectional_mean_ps,
                        flow.bidirectional_stddev_ps,
                        flow.bidirectional_max_ps,
                        flow.src2dst_min_ps,
                        flow.src2dst_mean_ps,
                        flow.src2dst_stddev_ps,
                        flow.src2dst_max_ps,
                        flow.dst2src_min_ps,
                        flow.dst2src_mean_ps,
                        flow.dst2src_stddev_ps,
                        flow.dst2src_max_ps,
                        flow.bidirectional_min_piat_ms,
                        flow.bidirectional_mean_piat_ms,
                        flow.bidirectional_stddev_piat_ms,
                        flow.bidirectional_max_piat_ms,
                        flow.src2dst_min_piat_ms,
                        flow.src2dst_mean_piat_ms,
                        flow.src2dst_stddev_piat_ms,
                        flow.src2dst_max_piat_ms,
                        flow.dst2src_min_piat_ms,
                        flow.dst2src_mean_piat_ms,
                        flow.dst2src_stddev_piat_ms,
                        flow.dst2src_max_piat_ms,
                        flow.bidirectional_syn_packets,
                        flow.bidirectional_cwr_packets,
                        flow.bidirectional_ece_packets,
                        flow.bidirectional_urg_packets,
                        flow.bidirectional_ack_packets,
                        flow.bidirectional_psh_packets,
                        flow.bidirectional_rst_packets,
                        flow.bidirectional_fin_packets,
                        flow.src2dst_syn_packets,
                        flow.src2dst_cwr_packets,
                        flow.src2dst_ece_packets,
                        flow.src2dst_urg_packets,
                        flow.src2dst_ack_packets,
                        flow.src2dst_psh_packets,
                        flow.src2dst_rst_packets,
                        flow.src2dst_fin_packets,
                        flow.dst2src_syn_packets,
                        flow.dst2src_cwr_packets,
                        flow.dst2src_ece_packets,
                        flow.dst2src_urg_packets,
                        flow.dst2src_ack_packets,
                        flow.dst2src_psh_packets,
                        flow.dst2src_rst_packets,
                        flow.dst2src_fin_packets,
                    ]
                ).reshape((1, -1))

                writer.writerow(flow_features)

                # Normalize the values.
                flow_features_normalized = scaler.transform(X=flow_features)

                # Classify.
                prediction = clf.predict(X=flow_features_normalized)[0]
                analyzed_flows[LABELS[prediction]] += 1
                print(
                    f"Flow {sum(analyzed_flows.values())} - {flow.src_ip}:{flow.src_port} --> {flow.dst_ip}:{flow.dst_port}. Prediction: {LABELS[prediction]}."
                )
        finally:
            print(f"\n")
            print(f"Total flows analyzed: {sum(analyzed_flows.values())}.")
            print(f"{analyzed_flows}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i", "--interface", type=str, help="Specify the network interface."
    )
    args = parser.parse_args()

    if not args.interface:
        print("Please provide a network interface using the -i or --interface option.")
        exit()

    run(args.interface)
