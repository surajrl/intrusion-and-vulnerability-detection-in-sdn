from nfstream import NFStreamer
from nfstream.flow import NFlow
import numpy as np
import pickle
import argparse


def run(network_interface_name):
    labels = [
    'Benign',
    'DoS GoldenEye',
    'DoS Hulk',
    'DoS Slowhttptest',
    'DoS Slowloris'
    ]

    root_path = '/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/saved/'

    # Load the trained classifier.
    clf_filepath = root_path + '/model_fold1.pkl' # Enter filepath to the trained classifier.
    clf = pickle.load(open(clf_filepath, 'rb'))
    # Load the normalization scaler.
    scaler_filepath = root_path + '/scaler.pkl' # Enter filepath to the scaler used during the training of the classifier.
    scaler = pickle.load(open(scaler_filepath, 'rb'))
    # Load the features that were used for training.
    features_filepath = root_path + '/features.pkl' # Enter filepath to a list with the name of the features used by the classifier.
    features = pickle.load(open(features_filepath, 'rb'))

    my_streamer = NFStreamer(source=network_interface_name,
                            decode_tunnels=True,
                            bpf_filter=None, 
                            promiscuous_mode=True,
                            snapshot_length=1536,
                            idle_timeout=120,
                            active_timeout=5, # Set active timeout to 5 seconds.
                            accounting_mode=0,
                            udps=None,
                            n_dissections=20,
                            statistical_analysis=True, # Enable post-mortem flow statistical analysis.
                            splt_analysis=0,
                            n_meters=0,
                            max_nflows=0,
                            performance_report=0,
                            system_visibility_mode=0,
                            system_visibility_poll_ms=100)

    analyzed_flows = {
        'Benign': 0,
        'DoS GoldenEye': 0,
        'DoS Hulk': 0,
        'DoS Slowhttptest': 0,
        'DoS Slowloris': 0,
    }

    print(f'Starting to analyze flows on network interface {network_interface_name} ...')
    try:
        for flow in my_streamer:
            flow_feature_values = []
            # Extract only the required features.
            for feature, feature_value in zip(flow.keys(), flow.values()):
                if feature in features:
                    flow_feature_values.append(feature_value)

            # Normalize the values.
            flow_feature_values_normalized = scaler.transform(X=np.array(flow_feature_values).reshape(1, -1))

            # Classify.
            prediction = clf.predict(X=flow_feature_values_normalized)[0]
            analyzed_flows[labels[prediction]] += 1
            print(f'Flow {sum(analyzed_flows.values())} - {flow.src_ip}:{flow.src_port} --> {flow.dst_ip}:{flow.dst_port}. Prediction: {labels[prediction]}.')
    finally:
        print(f'\n')
        print(f'Total flows analyzed: {sum(analyzed_flows.values())}.')
        print(f'{analyzed_flows}')

if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--interface', type=str, help='Specify the network interface.')
    args = parser.parse_args()

    if not args.interface:
        print('Please provide a network interface using the -i or --interaface option.')
        exit()
    
    run(args.interface)