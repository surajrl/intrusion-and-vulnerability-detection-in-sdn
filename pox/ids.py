from pox.core import core
import pox.openflow.libopenflow_01 as of
import pox.log.color

import pandas as pd
import joblib
import numpy as np
import threading
from nfstream import NFStreamer, NFPlugin

log = core.getLogger()

ALL_FEATURES = [
    "expiration_id",
    "protocol",
    "ip_version",
    "bidirectional_duration_ms",
    "bidirectional_packets",
    "bidirectional_bytes",
    "src2dst_duration_ms",
    "src2dst_packets",
    "src2dst_bytes",
    "dst2src_duration_ms",
    "dst2src_packets",
    "dst2src_bytes",
    "bidirectional_min_ps",
    "bidirectional_mean_ps",
    "bidirectional_stddev_ps",
    "bidirectional_max_ps",
    "src2dst_min_ps",
    "src2dst_mean_ps",
    "src2dst_stddev_ps",
    "src2dst_max_ps",
    "dst2src_min_ps",
    "dst2src_mean_ps",
    "dst2src_stddev_ps",
    "dst2src_max_ps",
    "bidirectional_min_piat_ms",
    "bidirectional_mean_piat_ms",
    "bidirectional_stddev_piat_ms",
    "bidirectional_max_piat_ms",
    "src2dst_min_piat_ms",
    "src2dst_mean_piat_ms",
    "src2dst_stddev_piat_ms",
    "src2dst_max_piat_ms",
    "dst2src_min_piat_ms",
    "dst2src_mean_piat_ms",
    "dst2src_stddev_piat_ms",
    "dst2src_max_piat_ms",
    "bidirectional_syn_packets",
    "bidirectional_cwr_packets",
    "bidirectional_ece_packets",
    "bidirectional_ack_packets",
    "bidirectional_psh_packets",
    "bidirectional_rst_packets",
    "bidirectional_fin_packets",
    "src2dst_syn_packets",
    "src2dst_cwr_packets",
    "src2dst_ece_packets",
    "src2dst_ack_packets",
    "src2dst_psh_packets",
    "src2dst_rst_packets",
    "src2dst_fin_packets",
    "dst2src_syn_packets",
    "dst2src_cwr_packets",
    "dst2src_ece_packets",
    "dst2src_ack_packets",
    "dst2src_psh_packets",
    "dst2src_rst_packets",
    "dst2src_fin_packets",
]

FLOW_FEATURES = [
    "id",
    "expiration_id",
    "src_ip",  # 2
    "src_mac",
    "src_oui",
    "src_port",  # 5
    "dst_ip",  # 6
    "dst_mac",
    "dst_oui",
    "dst_port",  # 9
    "protocol",
    "ip_version",
    "vlan_id",
    "bidirectional_first_seen_ms",
    "bidirectional_last_seen_ms",
    "bidirectional_duration_ms",
    "bidirectional_packets",
    "bidirectional_bytes",
    "src2dst_first_seen_ms",
    "src2dst_last_seen_ms",
    "src2dst_duration_ms",
    "src2dst_packets",
    "src2dst_bytes",
    "dst2src_first_seen_ms",
    "dst2src_last_seen_ms",
    "dst2src_duration_ms",
    "dst2src_packets",
    "dst2src_bytes",
    "bidirectional_min_ps",
    "bidirectional_mean_ps",
    "bidirectional_stddev_ps",
    "bidirectional_max_ps",
    "src2dst_min_ps",
    "src2dst_mean_ps",
    "src2dst_stddev_ps",
    "src2dst_max_ps",
    "dst2src_min_ps",
    "dst2src_mean_ps",
    "dst2src_stddev_ps",
    "dst2src_max_ps",
    "bidirectional_min_piat_ms",
    "bidirectional_mean_piat_ms",
    "bidirectional_stddev_piat_ms",
    "bidirectional_max_piat_ms",
    "src2dst_min_piat_ms",
    "src2dst_mean_piat_ms",
    "src2dst_stddev_piat_ms",
    "src2dst_max_piat_ms",
    "dst2src_min_piat_ms",
    "dst2src_mean_piat_ms",
    "dst2src_stddev_piat_ms",
    "dst2src_max_piat_ms",
    "bidirectional_syn_packets",
    "bidirectional_cwr_packets",
    "bidirectional_ece_packets",
    "bidirectional_urg_packets",
    "bidirectional_ack_packets",
    "bidirectional_psh_packets",
    "bidirectional_rst_packets",
    "bidirectional_fin_packets",
    "src2dst_syn_packets",
    "src2dst_cwr_packets",
    "src2dst_ece_packets",
    "src2dst_urg_packets",
    "src2dst_ack_packets",
    "src2dst_psh_packets",
    "src2dst_rst_packets",
    "src2dst_fin_packets",
    "dst2src_syn_packets",
    "dst2src_cwr_packets",
    "dst2src_ece_packets",
    "dst2src_urg_packets",
    "dst2src_ack_packets",
    "dst2src_psh_packets",
    "dst2src_rst_packets",
    "dst2src_fin_packets",
]

# Scaler names.
BASE_SCALER = "base_scaler"
FS_SCALER = "fs_scaler"
RESAMP_SCALER = "resamp_scaler"
FS_RESAMP_SCALER = "fs-resamp_scaler"

# Model names.
DT_BASE = "dt_base"
DT_FS = "dt_fs"
DT_RESAMP = "dt_resamp"
DT_FS_RESAMP = "dt_fs-resamp"
XGBOOST_BASE = "xgb_base"
XGBOOST_FS = "xgb_fs"
XGBOOST_RESAMP = "xgb_resamp"
XGBOOST_FS_RESAMP = "xgb_fs-resamp"
RF_BASE = "rf_base"
RF_FS = "rf_fs"
RF_RESAMP = "rf_resamp"
RF_FS_RESAMP = "rf_fs-resamp"

# Attack being replayed.
ATTACK = "dos_goldeneye"


class ModelPrediction(NFPlugin):
    def on_init(self, packet, flow):
        flow.udps.all_features_to_predict = []
        flow.udps.fs_features_to_predict = []
        flow.udps.flow_features = []

    def on_expire(self, flow):
        # Extract the features from feature selection that will be used for prediction.
        to_predict = np.empty((0,))
        for key in flow.keys():
            if key in self.fs_features:
                to_predict = np.append(to_predict, getattr(flow, key))
        to_predict = to_predict.reshape((1, -1))
        flow.udps.fs_features_to_predict = to_predict

        # Extract all the features that will be used for prediction.
        to_predict = np.empty((0,))
        for key in flow.keys():
            if key in ALL_FEATURES:
                to_predict = np.append(to_predict, getattr(flow, key))
        to_predict = to_predict.reshape((1, -1))
        flow.udps.all_features_to_predict = to_predict

        # Extract all the features (not for prediction).
        flow.udps.flow_features = []
        for key in flow.keys():
            if key in FLOW_FEATURES:
                flow.udps.flow_features.append(getattr(flow, key))


class IDS(object):
    def __init__(self, connection):

        # Keep track of the connection to the switch so that we can send it messages.
        self.connection = connection
        # Bind our event listeners to the switch.
        self.connection.addListeners(self)

        # This table maps (switch,MAC-addr) pairs to the port on 'switch' at
        # which we last saw a packet *from* 'MAC-addr'.
        # (In this case, we use a Connection object for the switch.)
        self.table = {}

        self.features = []

        # Store the prediction from all model variations.
        self.base_model_predictions = []
        self.fs_model_predictions = []
        self.resamp_model_predictions = []
        self.fs_resamp_model_predictions = []

        self.analyzed_flows = {
            "base_model": {
                "benign": 0,
                "bot": 0,
                "ddos": 0,
                "dos_goldeneye": 0,
                "dos_hulk": 0,
                "dos_slowhttptest": 0,
                "dos_slowloris": 0,
                "ftp_patator": 0,
                "portscan": 0,
                "ssh_patator": 0,
            },
            "fs_model": {
                "benign": 0,
                "bot": 0,
                "ddos": 0,
                "dos_goldeneye": 0,
                "dos_hulk": 0,
                "dos_slowhttptest": 0,
                "dos_slowloris": 0,
                "ftp_patator": 0,
                "portscan": 0,
                "ssh_patator": 0,
            },
            "resamp_model": {
                "benign": 0,
                "bot": 0,
                "ddos": 0,
                "dos_goldeneye": 0,
                "dos_hulk": 0,
                "dos_slowhttptest": 0,
                "dos_slowloris": 0,
                "ftp_patator": 0,
                "portscan": 0,
                "ssh_patator": 0,
            },
            "fs_resamp_model": {
                "benign": 0,
                "bot": 0,
                "ddos": 0,
                "dos_goldeneye": 0,
                "dos_hulk": 0,
                "dos_slowhttptest": 0,
                "dos_slowloris": 0,
                "ftp_patator": 0,
                "portscan": 0,
                "ssh_patator": 0,
            },
        }

        self.base_model_name = XGBOOST_BASE
        self.fs_model_name = XGBOOST_FS
        self.resamp_model_name = XGBOOST_RESAMP
        self.fs_resamp_model_name = XGBOOST_FS_RESAMP

        try:
            self.ids_thread = threading.Thread(target=self.ids)
            self.ids_thread.start()
        except KeyboardInterrupt:
            pass

    def _handle_ConnectionDown(self, event):
        output_dir = (
            "/home/suraj/intrusion-and-vulnerability-detection-in-sdn/practical_demo/"
        )

        if len(self.features) > 0:
            log.info(f"Saving results to {output_dir} ...")
            # Save the features.
            pd.DataFrame(self.features).to_csv(
                output_dir + f"features_{ATTACK}.csv",
                index=False,
                header=False,
            )

            # Save the predictions.
            # pd.DataFrame(self.base_model_predictions).to_csv(
            #     output_dir + f"predictions_{self.base_model_name}_{attack}.csv",
            #     index=False,
            #     header=False,
            # )
            # pd.DataFrame(self.fs_model_predictions).to_csv(
            #     output_dir + f"predictions_{self.fs_model_name}_{attack}.csv",
            #     index=False,
            #     header=False,
            # )
            pd.DataFrame(self.resamp_model_predictions).to_csv(
                output_dir + f"predictions_{self.resamp_model_name}_{ATTACK}.csv",
                index=False,
                header=False,
            )
            # pd.DataFrame(self.fs_resamp_model_predictions).to_csv(
            #     output_dir + f"predictions_{self.fs_resamp_model_name}_{attack}.csv",
            #     index=False,
            #     header=False,
            # )
        for key, value in self.analyzed_flows.items():
            if any(num > 0 for num in list(value.values())):
                if key == "base_model":
                    log.info("Traffic Analysis using Base Model:")
                elif key == "fs_model":
                    log.info("Traffic Analysis using FS Model:")
                elif key == "resamp_model":
                    log.info("Traffic Analysis using Resamp Model:")
                elif key == "fs_resamp_model":
                    log.info("Traffic Analysis using FS Resamp Model:")
                for k, v in value.items():
                    if v > 0:
                        if k == "benign":
                            log.info(f"- Benign: {v}.")
                        elif k == "bot":
                            log.info(f"- Bot: {v}.")
                        elif k == "ddos":
                            log.info(f"- DDoS: {v}.")
                        elif k == "dos_goldeneye":
                            log.info(f"- DoS GoldenEye: {v}.")
                        elif k == "dos_hulk":
                            log.info(f"- DoS Hulk: {v}.")
                        elif k == "dos_slowhttptest":
                            log.info(f"- DoS SlowHTTPTest: {v}.")
                        elif k == "dos_slowloris":
                            log.info(f"- DoS Slowloris: {v}.")
                        elif k == "ftp_patator":
                            log.info(f"- FTP Patator: {v}.")
                        elif k == "portscan":
                            log.info(f"- Portscan: {v}.")
                        elif k == "ssh_patator":
                            log.info(f"- SSH Patator: {v}.")

    def ids(self):
        log.info("IDS started ...")

        # Define the classes categorical label.
        class_labels = [
            "benign",
            "bot",
            "ddos",
            "dos_goldeneye",
            "dos_hulk",
            "dos_slowhttptest",
            "dos_slowloris",
            "ftp_patator",
            "portscan",
            "ssh_patator",
        ]

        # Load the fitted model and scaler.
        root_path = "/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/saved"

        base_model = joblib.load(root_path + f"/{self.base_model_name}.joblib")
        fs_model = joblib.load(root_path + f"/{self.fs_model_name}.joblib")
        resamp_model = joblib.load(root_path + f"/{self.resamp_model_name}.joblib")
        fs_resamp_model = joblib.load(
            root_path + f"/{self.fs_resamp_model_name}.joblib"
        )

        base_scaler = joblib.load(root_path + f"/{BASE_SCALER}.joblib")
        fs_scaler = joblib.load(root_path + f"/{FS_SCALER}.joblib")
        resamp_scaler = joblib.load(root_path + f"/{RESAMP_SCALER}.joblib")
        fs_resamp_scaler = joblib.load(root_path + f"/{FS_RESAMP_SCALER}.joblib")

        # log.info(f"Using model {self.model_name} with scaler {self.scaler_name}.")

        # Load the features from feature selection.
        with open(
            f"/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/feature_selection.txt",
            "r",
        ) as f:
            fs_features = f.read().splitlines()

        # Start analyzing the flows.
        flow_streamer = NFStreamer(
            source="s1-eth1",
            decode_tunnels=False,  # Disable GTP/CAPWAP/TZSP tunnels decoding.
            bpf_filter=None,
            promiscuous_mode=True,  # Enable promiscuous capture mode.
            snapshot_length=1536,  # Control packet slicing size (truncation) in bytes.
            idle_timeout=120,  # Flows that are idle (no packets received) for more than 120 seconds are expired.
            active_timeout=5,  # Flows that are active for more than <active_timeout> seconds are expired.
            accounting_mode=3,  # Accounting mode that will be used to report bytes related features, 3: payload.
            udps=ModelPrediction(fs_features=fs_features),
            n_dissections=0,  # Disable L7 visibility feature.
            statistical_analysis=True,  # Enable post-mortem flow statistical analysis.
            splt_analysis=0,
            n_meters=0,
            max_nflows=0,
            performance_report=0,
            system_visibility_mode=0,
            system_visibility_poll_ms=100,
        )

        def notify_prediction(message, prediction):
            if prediction != 0:
                log.warning(message)
            else:
                log.info(message)

        try:
            for flow in flow_streamer:
                # Extract all the flow features
                self.features.append(flow.udps.flow_features)

                # Predict using the base model.
                # prediction = base_model.predict(
                #     base_scaler.transform(flow.udps.all_features_to_predict)
                # )[0]
                # self.base_model_predictions.append(class_labels[prediction])
                # self.analyzed_flows["base_model"][class_labels[prediction]] += 1
                # message = f"[Base Model] Flow {len(self.base_model_predictions)}. Prediction: {class_labels[prediction]}."
                # notify_prediction(message, prediction)

                # # Predict using the fs model.
                # prediction = fs_model.predict(
                #     fs_scaler.transform(flow.udps.fs_features_to_predict)
                # )[0]
                # self.analyzed_flows["fs_model"][class_labels[prediction]] += 1
                # self.fs_model_predictions.append(class_labels[prediction])
                # message = f"[FS Model] Flow {len(self.fs_model_predictions)}. Prediction: {class_labels[prediction]}."
                # notify_prediction(message, prediction)

                # # Predict using the resamp model.
                prediction = resamp_model.predict(
                    resamp_scaler.transform(flow.udps.all_features_to_predict)
                )[0]
                self.analyzed_flows["resamp_model"][class_labels[prediction]] += 1
                self.resamp_model_predictions.append(class_labels[prediction])
                src_ip = flow.udps.flow_features[2]
                src_port = flow.udps.flow_features[5]
                dst_ip = flow.udps.flow_features[6]
                dst_port = flow.udps.flow_features[9]
                traffic_type_name = class_labels[prediction]

                message = f"Flow from {src_ip}:{src_port} to {dst_ip}:{dst_port} detected as {traffic_type_name}."

                notify_prediction(message, prediction)

                # # Predict using the fs_resamp model.
                # prediction = fs_resamp_model.predict(
                #     fs_resamp_scaler.transform(flow.udps.fs_features_to_predict)
                # )[0]
                # self.fs_resamp_model_predictions.append(class_labels[prediction])
                # self.analyzed_flows["fs_resamp_model"][class_labels[prediction]] += 1
                # message = f"[FS Resamp Model] Flow {len(self.fs_resamp_model_predictions)}. Prediction: {class_labels[prediction]}."
                # notify_prediction(message, prediction)
        except KeyboardInterrupt:
            pass

    # Handle messages the switch has sent us because it has no matching rule.
    def _handle_PacketIn(self, event):
        packet = event.parsed

        # Learn the source
        self.table[(event.connection, packet.src)] = event.port

        dst_port = self.table.get((event.connection, packet.dst))

        if dst_port is None:
            # We don't know where the destination is yet. So, we'll just
            # send the packet out all ports (except the one it came in on!)
            # and hope the destination is out there somewhere. :)
            msg = of.ofp_packet_out(data=event.ofp)
            msg.actions.append(of.ofp_action_output(port=of.OFPP_FLOOD))
            self.connection.send(msg)
        else:
            # Since we know the switch ports for both the source and dest
            # MACs, we can install rules for both directions.
            msg = of.ofp_flow_mod()
            msg.match.dl_dst = packet.src
            msg.match.dl_src = packet.dst
            msg.actions.append(of.ofp_action_output(port=event.port))
            self.connection.send(msg)

            # This is the packet that just came in -- we want to
            # install the rule and also resend the packet.
            msg = of.ofp_flow_mod()
            msg.data = event.ofp  # Forward the incoming packet
            msg.match.dl_src = packet.src
            msg.match.dl_dst = packet.dst
            msg.actions.append(of.ofp_action_output(port=dst_port))
            self.connection.send(msg)

            log.debug("Installing %s <-> %s" % (packet.src, packet.dst))


def launch():
    # Logger configuration.
    pox.log.color.launch()
    pox.log.launch(
        format="[@@@bold%(levelname)s@@@reset][@@@bold%(asctime)s@@@reset][@@@bold%(filename)s@@@reset]: %(message)s"
    )

    def start_switch_connection(event):
        # log.info("Controlling %s" % (event.connection,))
        IDS(event.connection)

    # Start connection of the controller with the switch.
    core.openflow.addListenerByName("ConnectionUp", start_switch_connection)
