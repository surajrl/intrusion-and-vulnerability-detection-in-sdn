from pox.core import core
import pox.openflow.libopenflow_01 as of
import pox.log.color

import pandas as pd
import joblib
import numpy as np
import threading
from nfstream import NFStreamer, NFPlugin

log = core.getLogger()

FLOW_FEATURES_TO_PREDICT = [
    "expiration_id",
    "protocol",
    "ip_version",
    "bidirectional_duration_ms",
    "bidirectional_packets",
    "bidirectional_bytes",
    "src2dst_duration_ms",
    "src2dst_packets",
    "src2dst_bytes",
    "dst2src_duration_ms",
    "dst2src_packets",
    "dst2src_bytes",
    "bidirectional_min_ps",
    "bidirectional_mean_ps",
    "bidirectional_stddev_ps",
    "bidirectional_max_ps",
    "src2dst_min_ps",
    "src2dst_mean_ps",
    "src2dst_stddev_ps",
    "src2dst_max_ps",
    "dst2src_min_ps",
    "dst2src_mean_ps",
    "dst2src_stddev_ps",
    "dst2src_max_ps",
    "bidirectional_min_piat_ms",
    "bidirectional_mean_piat_ms",
    "bidirectional_stddev_piat_ms",
    "bidirectional_max_piat_ms",
    "src2dst_min_piat_ms",
    "src2dst_mean_piat_ms",
    "src2dst_stddev_piat_ms",
    "src2dst_max_piat_ms",
    "dst2src_min_piat_ms",
    "dst2src_mean_piat_ms",
    "dst2src_stddev_piat_ms",
    "dst2src_max_piat_ms",
    "bidirectional_syn_packets",
    "bidirectional_cwr_packets",
    "bidirectional_ece_packets",
    "bidirectional_ack_packets",
    "bidirectional_psh_packets",
    "bidirectional_rst_packets",
    "bidirectional_fin_packets",
    "src2dst_syn_packets",
    "src2dst_cwr_packets",
    "src2dst_ece_packets",
    "src2dst_ack_packets",
    "src2dst_psh_packets",
    "src2dst_rst_packets",
    "src2dst_fin_packets",
    "dst2src_syn_packets",
    "dst2src_cwr_packets",
    "dst2src_ece_packets",
    "dst2src_ack_packets",
    "dst2src_psh_packets",
    "dst2src_rst_packets",
    "dst2src_fin_packets",
]

FLOW_FEATURES = [
    "id",
    "expiration_id",
    "src_ip",
    "src_mac",
    "src_oui",
    "src_port",
    "dst_ip",
    "dst_mac",
    "dst_oui",
    "dst_port",
    "protocol",
    "ip_version",
    "vlan_id",
    "bidirectional_first_seen_ms",
    "bidirectional_last_seen_ms",
    "bidirectional_duration_ms",
    "bidirectional_packets",
    "bidirectional_bytes",
    "src2dst_first_seen_ms",
    "src2dst_last_seen_ms",
    "src2dst_duration_ms",
    "src2dst_packets",
    "src2dst_bytes",
    "dst2src_first_seen_ms",
    "dst2src_last_seen_ms",
    "dst2src_duration_ms",
    "dst2src_packets",
    "dst2src_bytes",
    "bidirectional_min_ps",
    "bidirectional_mean_ps",
    "bidirectional_stddev_ps",
    "bidirectional_max_ps",
    "src2dst_min_ps",
    "src2dst_mean_ps",
    "src2dst_stddev_ps",
    "src2dst_max_ps",
    "dst2src_min_ps",
    "dst2src_mean_ps",
    "dst2src_stddev_ps",
    "dst2src_max_ps",
    "bidirectional_min_piat_ms",
    "bidirectional_mean_piat_ms",
    "bidirectional_stddev_piat_ms",
    "bidirectional_max_piat_ms",
    "src2dst_min_piat_ms",
    "src2dst_mean_piat_ms",
    "src2dst_stddev_piat_ms",
    "src2dst_max_piat_ms",
    "dst2src_min_piat_ms",
    "dst2src_mean_piat_ms",
    "dst2src_stddev_piat_ms",
    "dst2src_max_piat_ms",
    "bidirectional_syn_packets",
    "bidirectional_cwr_packets",
    "bidirectional_ece_packets",
    "bidirectional_urg_packets",
    "bidirectional_ack_packets",
    "bidirectional_psh_packets",
    "bidirectional_rst_packets",
    "bidirectional_fin_packets",
    "src2dst_syn_packets",
    "src2dst_cwr_packets",
    "src2dst_ece_packets",
    "src2dst_urg_packets",
    "src2dst_ack_packets",
    "src2dst_psh_packets",
    "src2dst_rst_packets",
    "src2dst_fin_packets",
    "dst2src_syn_packets",
    "dst2src_cwr_packets",
    "dst2src_ece_packets",
    "dst2src_urg_packets",
    "dst2src_ack_packets",
    "dst2src_psh_packets",
    "dst2src_rst_packets",
    "dst2src_fin_packets",
]

# Scaler names.
TRAIN_SCALER = "train_scaler"
TRAIN_SCALER_FS = "train-fs_scaler"
TRAIN_SCALER_RESAMP = "train-resamp_scaler"
TRAIN_SCALER_FS_RESAMP = "train-fs-resamp_scaler"

# Model names.
DT_TRAIN = "dt_train"
DT_TRAIN_FS = "dt_train-fs"
DT_TRAIN_RESAMP = "dt_train-resamp"
DT_TRAIN_FS_RESAMP = "dt_train-fs-resamp"
XGBOOST_TRAIN = "xgb_train"
XGBOOST_TRAIN_FS = "xgb_train-fs"
XGBOOST_TRAIN_RESAMP = "xgb_train-resamp"
XGBOOST_TRAIN_FS_RESAMP = "xgb_train-fs-resamp"
RF_TRAIN = "rf_train"
RF_TRAIN_FS = "rf_train_fs"
RF_TRAIN_RESAMP = "rf_train_resamp"
RF_TRAIN_FS_RESAMP = "rf_train_fs_resamp"


class ModelPrediction(NFPlugin):
    def on_init(self, packet, flow):
        flow.udps.flow_features_to_predict = []
        flow.udps.flow_features = []

    def on_expire(self, flow):
        # Extract the features that will be used for prediction.
        to_predict = np.empty((0,))
        for key in flow.keys():
            # TODO: update for feature selection case.
            if key in FLOW_FEATURES_TO_PREDICT:
                to_predict = np.append(to_predict, getattr(flow, key))
        to_predict = to_predict.reshape((1, -1))
        flow.udps.flow_features_to_predict = to_predict

        # Extract all the features of the flow.
        flow.udps.flow_features = []
        for key in flow.keys():
            if key in FLOW_FEATURES:
                flow.udps.flow_features.append(getattr(flow, key))


class IDS(object):
    def __init__(self, connection):

        # Keep track of the connection to the switch so that we can send it messages.
        self.connection = connection
        # Bind our event listeners to the switch.
        self.connection.addListeners(self)

        # This table maps (switch,MAC-addr) pairs to the port on 'switch' at
        # which we last saw a packet *from* 'MAC-addr'.
        # (In this case, we use a Connection object for the switch.)
        self.table = {}

        self.flow_features = []
        self.predictions = []
        self.analyzed_flows = {
            "benign": 0,
            "bot": 0,
            "ddos": 0,
            "dos_goldeneye": 0,
            "dos_hulk": 0,
            "dos_slowhttptest": 0,
            "dos_slowloris": 0,
            "ftp_patator": 0,
            "portscan": 0,
            "ssh_patator": 0,
        }

        self.scaler_name = TRAIN_SCALER
        self.model_name = DT_TRAIN

        try:
            self.ids_thread = threading.Thread(target=self.ids)
            self.ids_thread.start()
        except KeyboardInterrupt:
            pass

    def _handle_ConnectionDown(self, event):
        output_dir = "/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/sdn-emulation-results/"
        timestamp = pd.Timestamp.now().strftime("%Y-%m-%d_%H-%M-%S")

        log.info(f"Saving results to {output_dir} ...")
        # Save the flow features.
        if len(self.flow_features) > 0:
            pd.DataFrame(self.flow_features).to_csv(
                output_dir + f"features_{self.model_name}_{timestamp}.csv",
                index=False,
                header=False,
            )
        # Save the predictions.
        if len(self.predictions) > 0:
            pd.DataFrame(self.predictions).to_csv(
                output_dir + f"predictions_{self.model_name}_{timestamp}.csv",
                index=False,
                header=False,
            )

        log.info(f"Total flows analyzed: {sum(self.analyzed_flows.values())}.")
        if sum(self.analyzed_flows.values()) > 0:
            for key, value in self.analyzed_flows.items():
                log.info(f"{key} flows: {value}.")

    def ids(self):
        log.info("IDS started ...")

        # Define the classes categorical label.
        class_labels = [
            "benign",
            "bot",
            "ddos",
            "dos_goldeneye",
            "dos_hulk",
            "dos_slowhttptest",
            "dos_slowloris",
            "ftp_patator",
            "portscan",
            "ssh_patator",
        ]

        root_path = "/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/saved"
        # Load the fitted model and scaler.
        model = joblib.load(root_path + f"/{self.model_name}.joblib")
        scaler = joblib.load(root_path + f"/{self.scaler_name}.joblib")

        flow_streamer = NFStreamer(
            source="s1-eth1",
            decode_tunnels=False,  # Disable GTP/CAPWAP/TZSP tunnels decoding.
            bpf_filter=None,
            promiscuous_mode=True,  # Enable promiscuous capture mode.
            snapshot_length=1536,  # Control packet slicing size (truncation) in bytes.
            idle_timeout=120,  # Flows that are idle (no packets received) for more than 120 seconds are expired.
            active_timeout=5,  # Flows that are active for more than <active_timeout> seconds are expired.
            accounting_mode=3,  # Accounting mode that will be used to report bytes related features, 3: payload.
            udps=ModelPrediction(),
            n_dissections=0,  # Disable L7 visibility feature.
            statistical_analysis=True,  # Enable post-mortem flow statistical analysis.
            splt_analysis=0,
            n_meters=0,
            max_nflows=0,
            performance_report=0,
            system_visibility_mode=0,
            system_visibility_poll_ms=100,
        )

        for flow in flow_streamer:
            flow_features_to_predict_normalized = scaler.transform(
                flow.udps.flow_features_to_predict
            )
            prediction = model.predict(flow_features_to_predict_normalized)[0]

            self.predictions.append(class_labels[prediction])
            self.flow_features.append(flow.udps.flow_features)
            self.analyzed_flows[class_labels[prediction]] += 1

            message = f"Flow {sum(self.analyzed_flows.values())}. Prediction: {class_labels[prediction]}."
            if prediction != 0:
                log.warning(message)
            else:
                log.info(message)

    # Handle messages the switch has sent us because it has no matching rule.
    def _handle_PacketIn(self, event):
        packet = event.parsed

        # Learn the source
        self.table[(event.connection, packet.src)] = event.port

        dst_port = self.table.get((event.connection, packet.dst))

        if dst_port is None:
            # We don't know where the destination is yet. So, we'll just
            # send the packet out all ports (except the one it came in on!)
            # and hope the destination is out there somewhere. :)
            msg = of.ofp_packet_out(data=event.ofp)
            msg.actions.append(of.ofp_action_output(port=of.OFPP_FLOOD))
            self.connection.send(msg)
        else:
            # Since we know the switch ports for both the source and dest
            # MACs, we can install rules for both directions.
            msg = of.ofp_flow_mod()
            msg.match.dl_dst = packet.src
            msg.match.dl_src = packet.dst
            msg.actions.append(of.ofp_action_output(port=event.port))
            self.connection.send(msg)

            # This is the packet that just came in -- we want to
            # install the rule and also resend the packet.
            msg = of.ofp_flow_mod()
            msg.data = event.ofp  # Forward the incoming packet
            msg.match.dl_src = packet.src
            msg.match.dl_dst = packet.dst
            msg.actions.append(of.ofp_action_output(port=dst_port))
            self.connection.send(msg)

            log.debug("Installing %s <-> %s" % (packet.src, packet.dst))


def launch():
    # Logger configuration.
    pox.log.color.launch()
    pox.log.launch(
        format="[@@@bold%(levelname)s@@@reset][@@@bold%(asctime)s@@@reset][@@@bold%(filename)s@@@reset]: @@@bold%(message)s@@@normal"
    )

    def start_switch_connection(event):
        log.info("Controlling %s" % (event.connection,))
        IDS(event.connection)

    # Start connection of the controller with the switch.
    core.openflow.addListenerByName("ConnectionUp", start_switch_connection)
