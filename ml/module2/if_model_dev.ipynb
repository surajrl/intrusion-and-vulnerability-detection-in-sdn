{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label encoding**\n",
    "\n",
    "- **benign: 0**\n",
    "- **bot: 1**\n",
    "- **ddos: 2**\n",
    "- **dos_goldeneye: 3**\n",
    "- **dos_hulk: 4**\n",
    "- **dos_slowhttptest: 5**\n",
    "- **dos_slowloris: 6**\n",
    "- **ftp_patator: 7**\n",
    "- **portscan: 8**\n",
    "- **ssh_patator: 9**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cicids2017_df = pd.read_csv(\"./dataset/cicids2017-preprocessed.csv\", skipinitialspace=True)\n",
    "cicids2017_feature_selection_df = pd.read_csv(\"./dataset/cicids2017-feature-selection.csv\", skipinitialspace=True)\n",
    "print(f\"CIC-IDS 2017: {cicids2017_df.shape}\")\n",
    "print(f\"CIC-IDS 2017 with feature selection: {cicids2017_feature_selection_df.shape}\")\n",
    "cicids2017_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Module 2 - Anomaly Detection using Isolation Forest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cicids2017_feature_selection_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_df = df[df[\"label\"] == 0]\n",
    "attack_df = df[df[\"label\"] == 4]\n",
    "\n",
    "df = pd.concat([benign_df, attack_df])\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.loc[:, \"label\"] = df[\"label\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "df.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = df[df[\"label\"] == 0]\n",
    "attack = df[df[\"label\"] == 1]\n",
    "\n",
    "outlier_fraction = len(attack) / float(len(benign) + len(attack))\n",
    "\n",
    "print(outlier_fraction)\n",
    "print(f\"Benign instances: {len(benign)}\")\n",
    "print(f\"Attack instances: {len(attack)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split dependent and independent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"label\"]).values # Features\n",
    "y = df[\"label\"].values # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features = df.drop(columns=[\"label\"]).columns\n",
    "feature_importance_scores = np.zeros(X_scaled.shape[1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_scaled, y)):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Train a random forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=20, max_depth=None, bootstrap=False, n_jobs=-1, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred = rf.predict(X_val)\n",
    "\n",
    "    print(f\"Fold {fold} - Accuracy: {accuracy_score(y_val, y_val_pred)}\")\n",
    "\n",
    "    # Accumulate feature importance scores\n",
    "    feature_importance_scores += rf.feature_importances_\n",
    "\n",
    "feature_importance_scores /= 5 # divide by 5 to find average F.E.S.\n",
    "\n",
    "# Create a DataFrame to store feature importance scores\n",
    "feature_importance_df = pd.DataFrame({'feature': features, 'importance': feature_importance_scores})\n",
    "# Sort features by importance scores in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Keep features based on culumative importance\n",
    "cumulative_importance = 0\n",
    "features_selected = []\n",
    "indexes = []\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    if cumulative_importance >= 0.9:\n",
    "        break\n",
    "    indexes.append(index)\n",
    "    features_selected.append(row['feature'])\n",
    "    cumulative_importance += row['importance']\n",
    "\n",
    "print(f\"Features selected:\\n{features_selected}\")\n",
    "\n",
    "# NOTE: Need to rescale again!\n",
    "X_features_selected = df[features_selected].values  \n",
    "X_features_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_features_selected_scaled = scaler.fit_transform(X_features_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Fit Isolation Forest model\n",
    "if_clf = IsolationForest(random_state=42, n_jobs=-1)\n",
    "if_clf.fit(X_features_selected_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict anomaly scores\n",
    "anomaly_scores = if_clf.score_samples(X_features_selected_scaled)\n",
    "# y_pred = if_clf.predict(X_pca)\n",
    "# y_pred[y_pred == 1] = 0\n",
    "# y_pred[y_pred == -1] = 1\n",
    "\n",
    "# threshold = -0.33783911297874686\n",
    "threshold = -0.35\n",
    "y_pred = [1 if x <= threshold else 0 for x in anomaly_scores]\n",
    "\n",
    "cm_normalized = confusion_matrix(y, y_pred, normalize=\"true\")\n",
    "fp = cm_normalized[0][1]  # False Positive\n",
    "fn = cm_normalized[1][0]  # False Negative\n",
    "tp = cm_normalized[1][1]  # True Positive\n",
    "tn = cm_normalized[0][0]  # True Negative\n",
    "\n",
    "print(f\"\\nparams: {if_clf.get_params()}\")\n",
    "print(f\"Accuracy score: {accuracy_score(y, y_pred)}\")\n",
    "print(f\"tn: {tn:.4f} fp: {fp:.4f}\\nfn: {fn:.4f} tp: {tp:.4f}\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(y, y_pred)}\")\n",
    "\n",
    "# Plot histogram of anomaly scores\n",
    "plt.hist(anomaly_scores, bins=50, density=True, alpha=0.7, color='blue')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Anomaly Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the anomaly score for the true positives\n",
    "\n",
    "anomaly_scores_for_true_anomaly = []\n",
    "for i, label in enumerate(y):\n",
    "  if label == 1:\n",
    "    anomaly_scores_for_true_anomaly.append(anomaly_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(anomaly_scores_for_true_anomaly, dtype=np.float64).max())\n",
    "print(np.array(anomaly_scores_for_true_anomaly, dtype=np.float64).min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-parameter optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_features_selected.values)\n",
    "X_features_selected_scaled = scaler.transform(X_features_selected.values)\n",
    "\n",
    "# Hyper-parameter tuning\n",
    "\n",
    "# Define parameter grid search\n",
    "param_grid = {\n",
    "    'contamination': [0.1, 0.2, 0.3, 0.4, 0.5, outlier_fraction],\n",
    "    'max_samples': [256, 500, 1000, len(X_features_selected_scaled)],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_features': [5, 10, 15, len(features_selected)],\n",
    "    'bootstrap': [True],\n",
    "}\n",
    "\n",
    "# Perform grid search by iterating over all parameter combinations\n",
    "for contamination in param_grid['contamination']:\n",
    "    for max_samples in param_grid['max_samples']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            for max_features in param_grid['max_features']:\n",
    "                for bootstrap in param_grid['bootstrap']:\n",
    "                    # Initialize and train classifier with current parameters\n",
    "                    if_clf = IsolationForest(n_jobs=-1,\n",
    "                                            random_state=42, \n",
    "                                            contamination=contamination,\n",
    "                                            max_samples=max_samples, \n",
    "                                            n_estimators=n_estimators, \n",
    "                                            max_features=max_features, \n",
    "                                            bootstrap=bootstrap\n",
    "                                            )\n",
    "                    \n",
    "                    # Evaluate the classifier                 \n",
    "                    if_clf.fit(X_features_selected_scaled)\n",
    "\n",
    "                    y_pred = if_clf.predict(X_features_selected_scaled)\n",
    "                    y_pred[y_pred == 1] = 0\n",
    "                    y_pred[y_pred == -1] = 1\n",
    "\n",
    "                    cm_normalized = confusion_matrix(y, y_pred, normalize=\"true\")\n",
    "\n",
    "                    fp = cm_normalized[0][1]  # False Positive\n",
    "                    fn = cm_normalized[1][0]  # False Negative\n",
    "                    tp = cm_normalized[1][1]  # True Positive\n",
    "                    tn = cm_normalized[0][0]  # True Negative\n",
    "                    print(f\"\\nparams: {if_clf.get_params()}\")\n",
    "                    print(f\"tn: {tn:.4f} fp: {fp:.4f}\\nfp: {fp:.4f} tp: {tp:.4f}\")\n",
    "\n",
    "                    del if_clf # Releas memory just in case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "year3project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
