import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from skopt import gp_minimize
from skopt.space import Integer, Categorical, Real
from skopt.utils import use_named_args
from sklearn.metrics import f1_score
import time
import joblib
import os

# TO run in local environment
OUTPUT_DIR = "C:/development/intrusion-and-vulnerability-detection-in-sdn/ml/saved"
TRAIN_FILEPATH = "C:/development/intrusion-and-vulnerability-detection-in-sdn/ml/dataset/cicids2017-train.csv"
TRAIN_FS_FILEPATH = "C:/development/intrusion-and-vulnerability-detection-in-sdn/ml/dataset/cicids2017-train-fs.csv"
TRAIN_RESAMP_FILEPATH = "C:/development/intrusion-and-vulnerability-detection-in-sdn/ml/dataset/cicids2017-train-resamp.csv"
TRAIN_FS_RESAMP_FILEPATH = "C:/development/intrusion-and-vulnerability-detection-in-sdn/ml/dataset/cicids2017-train-fs-resamp.csv"

# To run in Google Colab
# OUTPUT_DIR = "/content/drive/MyDrive/trained-ml-models"
# TRAIN_FILEPATH = "./cicids2017-train.csv"
# TRAIN_FS_FILEPATH = "./cicids2017-train-fs.csv"
# TRAIN_RESAMP_FILEPATH = "./cicids2017-train-resamp.csv"
# TRAIN_FS_RESAMP_FILEPATH = "./cicids2017-train-fs-resamp.csv"

# Define hyper-parameters of the model to be tuned and their range
DT_SEARCH_SPACE = [
    Integer(5, 50, name="max_depth"),
    Integer(1, 20, name="max_features"),
    Integer(2, 11, name="min_samples_split"),
    Integer(1, 11, name="min_samples_leaf"),
    Categorical(["gini", "entropy"], name="criterion"),
]
RF_SEARCH_SPACE = [
    Integer(10, 100, name="n_estimators"),
    Integer(5, 50, name="max_depth"),
    Integer(1, 20, name="max_features"),
    Integer(2, 11, name="min_samples_split"),
    Integer(1, 11, name="min_samples_leaf"),
    Categorical(["gini", "entropy"], name="criterion"),
]
XGB_SEARCH_SPACE = [
    Real(0.01, 0.5, name="learning_rate"),
    Integer(10, 100, name="n_estimators"),
    Integer(3, 10, name="max_depth"),
]


def optimize_hyperparameters(X_train, X_val, y_train, y_val, search_space, model):
    @use_named_args(search_space)
    def objective(**params):
        model.set_params(**params)

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        f1_score_macro = f1_score(y_val, y_pred, average="macro")
        print(f"Hyper-parameters: {params} - F1 score (macro): {f1_score_macro}")
        return 1 - f1_score_macro

    t1 = time.time()
    res_gp = gp_minimize(objective, search_space, n_calls=50, verbose=True)
    t2 = time.time()

    print(f"\nTime taken: {t2-t1} seconds")
    print(f"Best score: {1-res_gp.fun}")
    print(f"Best hyper-parameters: {str(res_gp.x)}")

    return res_gp


def main():
    train_df = pd.read_csv(TRAIN_FILEPATH)
    print(f"{TRAIN_FILEPATH} loaded")
    train_fs_df = pd.read_csv(TRAIN_FS_FILEPATH)
    print(f"{TRAIN_FS_FILEPATH} loaded")
    train_resamp_df = pd.read_csv(TRAIN_RESAMP_FILEPATH)
    print(f"{TRAIN_RESAMP_FILEPATH} loaded")
    train_fs_resamp_df = pd.read_csv(TRAIN_FS_RESAMP_FILEPATH)
    print(f"{TRAIN_FS_RESAMP_FILEPATH} loaded")

    datasets_df = {
        "train": train_df,
        "train-fs": train_fs_df,
        "train-resamp": train_resamp_df,
        "train-fs-resamp": train_fs_resamp_df,
    }

    for df_name, df in datasets_df.items():
        print("\n" + "=" * 75 + "\n")
        print(f"{df_name}: {df.shape}")
        print(df["label"].value_counts())

        # Split into independent (X) and dependent (y) variables
        X = df.drop(columns=["label"]).values
        y = df["label"].values

        # Feature scaling
        scaler_filepath = f"{OUTPUT_DIR}/{df_name}_scaler.joblib"
        # Check if a scaler for this dataset already exists
        if not os.path.exists(scaler_filepath):
            scaler = StandardScaler()
            scaler.fit(X)
            X_scaled = scaler.transform(X)
            # Save the scaler
            joblib.dump(scaler, scaler_filepath)
            print(f"Scaler {df_name} saved to {scaler_filepath}")
        else:
            print(f"Scaler for {df_name} found")
            scaler = joblib.load(scaler_filepath)
            scaler.fit(X)
            X_scaled = scaler.transform(X)

        # Train/validation split
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"Train Set: {X_train.shape}\n{pd.Series(y_train).value_counts()}")
        print(f"Validation Set: {X_val.shape}\n{pd.Series(y_val).value_counts()}")

        # Model with default hyper-parameters (no optimization)
        models = {
            # "dt": tree.DecisionTreeClassifier(random_state=42),
            # "rf": RandomForestClassifier(random_state=42, n_jobs=-1),
            "xgb": XGBClassifier(random_state=42),
        }

        for model_name, model in models.items():
            if "dt" in model_name:
                search_space = DT_SEARCH_SPACE
            elif "rf" in model_name:
                search_space = RF_SEARCH_SPACE
            elif "xgb" in model_name:
                search_space = XGB_SEARCH_SPACE
            else:
                continue

            res_gp = optimize_hyperparameters(
                X_train=X_train,
                X_val=X_val,
                y_train=y_train,
                y_val=y_val,
                search_space=search_space,
                model=model,
            )

            if "dt" in model_name:
                optim_model = tree.DecisionTreeClassifier(
                    max_depth=res_gp.x[0],
                    max_features=res_gp.x[1],
                    min_samples_split=res_gp.x[2],
                    min_samples_leaf=res_gp.x[3],
                    criterion=res_gp.x[4],
                    random_state=42,
                )
            elif "rf" in model_name:
                optim_model = RandomForestClassifier(
                    n_estimators=res_gp.x[0],
                    max_depth=res_gp.x[1],
                    max_features=res_gp.x[2],
                    min_samples_split=res_gp.x[3],
                    min_samples_leaf=res_gp.x[4],
                    criterion=res_gp.x[5],
                    random_state=42,
                    n_jobs=-1,
                )
            elif "xgb" in model_name:
                optim_model = XGBClassifier(
                    learning_rate=res_gp.x[0],
                    n_estimators=res_gp.x[1],
                    max_depth=res_gp.x[2],
                    random_state=42,
                )
            else:
                continue

            # Train the model with the best hyper-parameters and save it
            optim_model.fit(X_train, y_train)
            output_file = f"{OUTPUT_DIR}/{model_name}_{df_name}.joblib"
            joblib.dump(optim_model, output_file)
            print(f"{model_name} model saved to {output_file}")


"""
ENTRY POINT
"""
main()
