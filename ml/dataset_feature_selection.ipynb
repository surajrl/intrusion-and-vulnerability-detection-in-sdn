{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./dataset/cicids2017-train-preprocessed.csv')\n",
    "\n",
    "print(f\"Train Set: {train_df.shape}\")\n",
    "print(train_df[\"label\"].value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection**\n",
    "\n",
    "**1. Supervised techniques**\n",
    "\n",
    "**1.1 Filter-based Approach**\n",
    "\n",
    "Filter-based feature selection approaches are based on data intrinsic attributes such as feature correlation or statistics. These approaches assess the value of each characteristic alone or in pairs without taking into account the performance of a particular learning algorithm.\n",
    "\n",
    "Filter-based approaches are computationally efficient and may be used with a variety of learning algorithms. However, because they do not account for the interaction between the features and the learning method, they may not always capture the ideal feature subset for a certain algorithm.\n",
    "\n",
    "- Information gain\n",
    "- Chi-square test\n",
    "- Fisher's score\n",
    "- Missing value ratio\n",
    "\n",
    "**1.2 Wrapper-based Approach**\n",
    "\n",
    "They assess the importance of features using a specific machine learning algorithm\n",
    "\n",
    "- Forward selection\n",
    "- Backward selection\n",
    "- Exhaustive feature selection\n",
    "- Recursive feature elimination\n",
    "\n",
    "**1.3 Embedded Approach**\n",
    "\n",
    "They include the feature selection process as part of the learning algorithm\n",
    "\n",
    "- Regularization\n",
    "- Random forest importance\n",
    "\n",
    "**2. Unsupervised techniques**\n",
    "\n",
    "- **Principal component analysis (PCA)**\n",
    "- **Independent component analysis (ICA)**\n",
    "- **NMF**\n",
    "- **t-SNE**\n",
    "- **Autoencoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into independent (X) and dependent (y) variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[\"label\"]).values\n",
    "y = train_df[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "ig = mutual_info_classif(X_scaled, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store the feature importance scores\n",
    "feature_scores = {}\n",
    "for i in range(len(train_df.columns)):\n",
    "    feature_scores[train_df.columns[i]] = ig[i-1]\n",
    "  \n",
    "# Sort the features by importance score in descending order\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance scores\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"{feature}: {score}\")\n",
    "\n",
    "# Plot a horizontal bar chart of the feature importance scores\n",
    "top_n = 30  # Plot only the top N features\n",
    "top_features = sorted_features[:top_n]\n",
    "\n",
    "# Plot a horizontal bar chart of the top N feature importance scores\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "y_pos = np.arange(len(top_features))\n",
    "ax.barh(y_pos, [score for feature, score in top_features], align=\"center\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([feature for feature, score in top_features])\n",
    "ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "ax.set_xlabel(\"Importance Score\")\n",
    "ax.set_title(f\"Top {top_n} Feature Importance Scores (Information Gain)\")\n",
    "\n",
    "# Add importance scores as labels on the horizontal bar chart\n",
    "for i, v in enumerate([score for feature, score in top_features]):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), color=\"black\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent cropping of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rfc.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = rfc.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Select the top n features\n",
    "top_n = 30\n",
    "top_indices = indices[:top_n]\n",
    "top_importances = importances[top_indices]\n",
    "\n",
    "for f in range(top_n):  # Use num_features instead of 10\n",
    "    print(f\"{train_df.columns[indices[f]]}: {importances[indices[f]]}\")\n",
    "\n",
    "# Plot the top n feature importances in a horizontal bar chart\n",
    "plt.barh(range(top_n), top_importances, align=\"center\")\n",
    "plt.yticks(range(top_n), train_df.columns[top_indices])\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Top {top_n} Feature Importance Scores (Random Forest Importance)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features = train_df.drop(columns=[\"label\"]).columns\n",
    "feature_importance_scores = np.zeros(X.shape[1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_scaled, y)):\n",
    "    X_train, X_val = X[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Train a random forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=20, max_depth=None, bootstrap=False, n_jobs=-1, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred = rf.predict(X_val)\n",
    "\n",
    "    print(f\"Fold {fold} - Accuracy: {accuracy_score(y_val, y_val_pred)}\")\n",
    "\n",
    "    # Accumulate feature importance scores\n",
    "    feature_importance_scores += rf.feature_importances_\n",
    "\n",
    "feature_importance_scores /= 5 # divide by 5 to find average F.E.S.\n",
    "\n",
    "# Create a DataFrame to store feature importance scores\n",
    "feature_importance_df = pd.DataFrame({'feature': features, 'importance': feature_importance_scores})\n",
    "# Sort features by importance scores in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Keep features based on culumative importance\n",
    "cumulative_importance = 0\n",
    "features_selected = []\n",
    "indexes = []\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    if cumulative_importance >= 0.9:\n",
    "        break\n",
    "    indexes.append(index)\n",
    "    features_selected.append(row['feature'])\n",
    "    cumulative_importance += row['importance']\n",
    "\n",
    "features_selected.append('label') # Add label column to the selected features\n",
    "features_selected_df = df[features_selected]  \n",
    "features_selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "feature_importance_df.plot(x='feature', y='importance', kind='barh')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.yticks(fontsize=4)  # Set the font size of the y-axis labels\n",
    "plt.show()\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected_df.to_csv('./dataset/cicids2017-rffi.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "year3project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
