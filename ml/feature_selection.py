import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def save_dataframe(x_train, y_train, x_test, y_test, features, name):
    X_train = pd.DataFrame(x_train)
    X_train.columns = features
    Y_train = pd.DataFrame(y_train)
    train_frame = [X_train, Y_train]
    train_final = pd.concat(train_frame, axis=1)
    train_final.to_csv(name + '_train.csv', index=False)
    print('Train dataset saved')

    X_test = pd.DataFrame(x_test)
    X_test.columns= features
    Y_test = pd.DataFrame(y_test)
    test_frame = [X_test,Y_test]
    test_final = pd.concat(test_frame, axis=1)
    test_final.to_csv(name + '_test.csv', index=False)
    print('Test dataset saved')

train_filepath = '/home/suraj/sdn/cleaned_dataset_train.csv'
test_filepath = '/home/suraj/sdn/cleaned_dataset_test.csv'
train_df = pd.read_csv(train_filepath, skipinitialspace=True)
test_df = pd.read_csv(test_filepath, skipinitialspace=True)

X_train = train_df.drop(train_df.columns[-1], axis=1)
y_train = train_df[train_df.columns[-1]]

X_test = test_df.drop(test_df.columns[-1], axis=1)
y_test = test_df[test_df.columns[-1]]

"""
Method for random forest feature importance
* Uses 5-fold cross validation
* Keeps features until cumulative importance >= 0.90
"""
def random_forest_feature_importance():
    features = X_train.columns
    feature_importance_scores = np.zeros(X_train.shape[1])

    scaler = preprocessing.StandardScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

    for train_index, val_index in skf.split(X_train, y_train):
        rf = RandomForestClassifier(
            n_estimators=20,
            max_depth=None,
            bootstrap=False,
            n_jobs=-1,
            random_state=0
        )

        rf.fit(X_train_scaled[train_index], y_train[train_index])
        y_val_pred = rf.predict(X_train_scaled[val_index])
        accuracy = accuracy_score(y_train[val_index], y_val_pred)

        feature_importance_scores += rf.feature_importances_

    feature_importance_scores /= 5

    feature_importance_df = pd.DataFrame(
        {
            'feature': features,
            'importance': feature_importance_scores
        })

    feature_importance_df = feature_importance_df.sort_values(
        by='importance',
        ascending=False
    )

    cumulative_importance = 0
    selected_features = []
    indexes = []
    for index, row in feature_importance_df.iterrows():
        if cumulative_importance >= 0.9:
            break
        indexes.append(index)
        selected_features.append(row['feature'])
        cumulative_importance += row['importance']
    
    return selected_features

selected_features = random_forest_feature_importance()
print('Selected Features')
print(selected_features)

X_train = X_train.loc[:, selected_features]
X_test = X_test.loc[:, selected_features]

save_dataframe(X_train, y_train, X_test, y_test, selected_features, 'reduced_dataset')