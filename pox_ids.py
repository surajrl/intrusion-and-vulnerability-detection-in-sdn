from pox.core import core
import pox.log.color
import pox.openflow.libopenflow_01 as of
from pox.lib.packet import ethernet
from pox.lib.recoco import Timer

import json
def convert_to_str(value):
    try:
        json.dumps(value)
        return value
    except TypeError:
        return str(value)

from flowmeter_v2 import constants
from flowmeter_v2.flow import Flow
from flowmeter_v2.utils import PacketDirection, get_packet_flow_key

import pickle

log = core.getLogger()

labels = [
        'Benign',
        'Bot',
        'DDoS',
        'DoS_GoldenEye',
        'DoS_Hulk',
        'DoS_SlowHTTPTest',
        'DoS_Slowloris',
        'FTP_Patator',
        'Heartbleed',
        'Infiltration',
        'Port_Scan',
        'SSH_Patator', 
        'Web_Attack_Brute_Force',
        'Web_Attack_SQL_Injection',
        'Web_Attack_XSS'
        ]

class POX_IDS(object):
    """
    A POX_IDS object is created for each switch that connects.
    A connection object for that switch is passed to the __init__ function.
    """

    def __init__ (self, connection):
        # Keep track of the connection to the switch so that we can send it messages.
        self.connection = connection

        # Bind our event listeners to the switch.
        connection.addListeners(self)

        # Track which ethernet address is on which switch port (keys are MACs, values are ports).
        self.mac_to_port = {}

        # Track all the network flows.
        self.flows = {}

        # Track the number of packets received.
        self.packets_count = 0

        # Track the number of malicious packets.
        self.malicious_packets_count = 0

        # self.fp = open('/home/suraj/intrusion-and-vulnerability-detection-in-sdn/flow_data.json', 'a')

        # Load the trained classifier.
        clf_filepath = '/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/dt_model.pkl'
        self.clf = pickle.load(open(clf_filepath, 'rb'))
        # Load the normalization scaler.
        scaler_filepath = '/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/scaler.pkl'
        self.scaler = pickle.load(open(scaler_filepath, 'rb'))
        # Load the required features.
        features_filepath = '/home/suraj/intrusion-and-vulnerability-detection-in-sdn/ml/features.pkl'
        self.features = pickle.load(open(features_filepath, 'rb'))

        Timer(10, self._handle_timer, recurring=True)

    def forward_packet(self, eth_pkt: ethernet, event):

        if eth_pkt.src not in self.mac_to_port:
            # Learn the port for the source MAC address.
            self.mac_to_port[eth_pkt.src] = event.ofp.in_port
            log.info(f'Learning MAC address {eth_pkt.src} to port {event.ofp.in_port}')

        # Port of the destination MAC address not known -- flooding.
        if eth_pkt.dst not in self.mac_to_port:
            msg = of.ofp_packet_out()
            msg.actions.append(of.ofp_action_output(port=of.OFPP_ALL))
            msg.data = event.ofp
            self.connection.send(msg)
        # Port of the destination MAC address known.
        else:
            msg = of.ofp_packet_out()
            msg.actions.append(of.ofp_action_output(port=self.mac_to_port[eth_pkt.dst]))
            msg.data = event.ofp
            self.connection.send(msg)

            # Install entry on table of the switch.
            log.info(f'Installing flow entry ...')
            msg = of.ofp_flow_mod()
            msg.match.dl_dst = eth_pkt.dst
            msg.actions.append(of.ofp_action_output(port=self.mac_to_port[eth_pkt.dst]))
            self.connection.send(msg)

    def _handle_ConnectionDown(self, event):
        log.info(f'Total packets received: {self.packets_count} of which {self.malicious_packets_count} where malicious.')

    def _handle_timer(self):
        # Send message to request for stats.
        flow_stats_request = of.ofp_stats_request(body=of.ofp_flow_stats_request())
        port_stats_request = of.ofp_stats_request(body=of.ofp_port_stats_request())
        aggregate_flow_stats_request = of.ofp_stats_request(body=of.ofp_aggregate_stats_request())
        table_stats_request = of.ofp_stats_request(body=of.ofp_table_stats_request())

        # ONLY TRIGGER STATS REQUEST WHEN NEW PACKET ARRIVES ?

        self.connection.send(flow_stats_request)
        self.connection.send(port_stats_request)
        self.connection.send(aggregate_flow_stats_request)
        self.connection.send(table_stats_request)
        
        log.info(f'\n\nMonitoring traffic ...')
        
    def _handle_PortStatsReceived(self, event):
        log.info(f'PortStatsReceived.')
        for i, stat in enumerate(event.stats):
            print(f'\nPortStat {i+1}')
            print(stat.show())
    
    def _handle_TableStatsReceived(self, event):
        log.info(f'TableStatsReceived.')
        for stat in event.stats:
            if stat.active_count != 0 or stat.lookup_count != 0 or stat.matched_count != 0:
                print(stat.show())

    def _handle_AggregateFlowStatsReceived(self, event):
        log.info(f'AggregateFlowStatsReceived.')
        print(event.stats.show())
    
    def _handle_FlowStatsReceived(self, event):
        log.info(f'FlowStatsReceived.')
        for i, stat in enumerate(event.stats):
            print(f'\nFlowStat {i+1}')
            print(stat.show())

    """ Handle packet in from the switch. The switch will send a packet if it has not table rule for that packet.
    """
    def _handle_PacketIn(self, event):
        log.info(f'PacketIn - src: {event.parsed.src} and dst: {event.parsed.dst}.')
         
        eth_pkt = event.parsed
        
        # Forward the packet.
        self.forward_packet(eth_pkt=eth_pkt, event=event)                     
        
        count = 0
        # Start by assuming the packet is in the forward direction.
        direction = PacketDirection.FORWARD

        # Check if the flow exists.
        try:
            packet_flow_key = get_packet_flow_key(eth_pkt, direction)
            flow = self.flows.get((packet_flow_key, count))
        except Exception:
            # Raised if the packet is not TCP or UDP.
            return

        self.packets_count += 1

        # If there is no forward flow with a count of 0, check if there is one of it in reverse.
        if flow is None:
            direction = PacketDirection.REVERSE
            packet_flow_key = get_packet_flow_key(eth_pkt, direction)
            flow = self.flows.get((packet_flow_key, count))
    
        # Create a new flow if no flow exists.
        if flow is None:
            direction = PacketDirection.FORWARD
            flow = Flow(eth_pkt, direction)
            packet_flow_key = get_packet_flow_key(eth_pkt, direction)
            self.flows[(packet_flow_key, count)] = flow

        # Create a new flow if the packet exists in the flow but it is sent after too much time, then it is part of a new flow.
        elif (eth_pkt.time - flow.latest_timestamp) > constants.EXPIRED_UPDATE:
            expired =    constants.EXPIRED_UPDATE
            while (eth_pkt.time - flow.latest_timestamp) > expired:
                    count += 1
                    expired +=    constants.EXPIRED_UPDATE
                    flow = self.flows.get((packet_flow_key, count))

                    if flow is None:
                        flow = Flow(eth_pkt, direction)
                        self.flows[(packet_flow_key, count)] = flow
                        break
                    
        # If it has a FIN flag then early collect flow.
        elif eth_pkt.find('tcp').FIN:
            flow.add_packet(eth_pkt, direction)
            self.garbage_collect(eth_pkt.time)
            return

        flow.add_packet(eth_pkt, direction)
        
        # TEST BY GENERATING A DATASET WITH KNOWN LABELS.
        # IN THE ULAK DATASET, IS EACH RECORD A PACKET DATA? OR FROM AN ENTIRE FLOW, MEANING THAT I HAVE TO WAIT FOR THE FLOW TO FINISH BEFORE ANALYZINF IT?

        # MACHINE LEARNING CLASSIFICATION.
        # Extract only the values of the features required by the model.
    #    flow_data = flow.get_data()
    #    converted_flow_data = { k: convert_to_str(value=v) for k, v in flow_data.items()}
    #    json.dump(converted_flow_data, self.fp)
    #    self.fp.write('\n')
    #    features_values = []
    #    for k, v in flow_data.items():
    #        if k in self.features:
    #                features_values.append(v)
    #     # Normalize the values.
    #    features_values_normalized = self.scaler.transform(np.array(features_values).reshape(1, -1))
    #    # Classify.
    #    prediction = self.clf.predict(X=features_values_normalized)[0]
    #    if prediction != 0:
    #        self.malicious_packets_count += 1
    #        log.warn(f'Packet {self.packets_count} - Prediction: {labels[prediction]}')
    #    log.info(f'Packet {self.packets_count} - Prediction: {labels[prediction]}')
        
        # If the number of packets collected reaches GARBAGE_COLLECT_PACKETS or the flow is lasting too long, collect flow.
        if self.packets_count %    constants.GARBAGE_COLLECT_PACKETS == 0 or flow.duration > 120:
                        self.garbage_collect(eth_pkt.time) 

    def garbage_collect(self, latest_time) -> None:
        # TODO: Garbage collection / feature extraction should have a separate thread
        log.info(f"Garbage Collection Began. Flows = {len(self.flows)}")

        for k in list(self.flows.keys()):
                flow = self.flows.get(k)

                if (
                        latest_time is not None
                        and latest_time - flow.latest_timestamp <    constants.EXPIRED_UPDATE
                        and flow.duration < 90
                ):
                        continue

                # TODO: Save into a CSV file
                log.info(flow.get_data())
                del self.flows[k]
        
        log.info(f"Garbage Collection Finished. Flows = {len(self.flows)}")


def launch ():
    """
    Starts the component
    """
    
    # Logger.
    pox.log.color.launch()
    pox.log.launch(format='[@@@bold%(levelname)s@@@reset][@@@bold%(asctime)s@@@reset][@@@bold%(filename)s@@@reset]: @@@bold%(message)s@@@normal')
    
    def handle_ConnectionUp(event):
        log.info(f'Controlling {str(event.connection,)}')
        return POX_IDS(event.connection)

    # Start connection with the switch.
    core.openflow.addListenerByName('ConnectionUp', handle_ConnectionUp)
